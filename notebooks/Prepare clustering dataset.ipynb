{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../flaubert_token_classification.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "from transformers import (\n",
    "    TFFlaubertForSequenceClassification,\n",
    "    FlaubertTokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_FOLDER = os.path.abspath(os.path.join(os.getcwd(), os.pardir)) + \"/\"\n",
    "MODELS_PATH = ROOT_FOLDER + \"models/\"\n",
    "DATASET_PATH = ROOT_FOLDER + \"dataset/custom_dataset/\"\n",
    "SEQUENCE_LENGTH = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"ner\": TFFlaubertForTokenClassification.from_pretrained(MODELS_PATH + \"ner\"),\n",
    "    \"pos\": TFFlaubertForTokenClassification.from_pretrained(MODELS_PATH + \"pos\"),\n",
    "    \"categorisation\": TFFlaubertForSequenceClassification.from_pretrained(MODELS_PATH + \"categorisation\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = FlaubertTokenizer.from_pretrained(\"jplu/tf-flaubert-base-cased\")\n",
    "SEQUENCE_LENGTH = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing example 0 of 553030 :  0.0%\n",
      "Elapsed time is 0.000998 seconds.\n",
      "Writing example 100 of 553030 :  0.02%\n",
      "Elapsed time is 86.205768 seconds.\n",
      "Writing example 200 of 553030 :  0.04%\n",
      "Elapsed time is 176.172176 seconds.\n",
      "Writing example 300 of 553030 :  0.05%\n",
      "Elapsed time is 264.363220 seconds.\n",
      "Writing example 400 of 553030 :  0.07%\n",
      "Elapsed time is 359.242074 seconds.\n",
      "Writing example 500 of 553030 :  0.09%\n",
      "Elapsed time is 452.591935 seconds.\n",
      "Writing example 600 of 553030 :  0.11%\n",
      "Elapsed time is 537.975520 seconds.\n",
      "Writing example 700 of 553030 :  0.13%\n",
      "Elapsed time is 621.078281 seconds.\n",
      "Writing example 800 of 553030 :  0.14%\n",
      "Elapsed time is 703.591844 seconds.\n",
      "Writing example 900 of 553030 :  0.16%\n",
      "Elapsed time is 799.414015 seconds.\n",
      "Writing example 1000 of 553030 :  0.18%\n",
      "Elapsed time is 901.373733 seconds.\n",
      "Writing example 1100 of 553030 :  0.2%\n",
      "Elapsed time is 1003.186386 seconds.\n",
      "Writing example 1200 of 553030 :  0.22%\n",
      "Elapsed time is 1095.906729 seconds.\n",
      "Writing example 1300 of 553030 :  0.24%\n",
      "Elapsed time is 1181.725701 seconds.\n",
      "Writing example 1400 of 553030 :  0.25%\n",
      "Elapsed time is 1265.215532 seconds.\n",
      "Writing example 1500 of 553030 :  0.27%\n",
      "Elapsed time is 1355.347734 seconds.\n"
     ]
    }
   ],
   "source": [
    "writer = tf.io.TFRecordWriter(\"./test_caching.tf_record\")\n",
    "\n",
    "from time import time\n",
    "t1 = time()\n",
    "\n",
    "def create_int_feature(values):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))\n",
    "\n",
    "def create_float_feature(values):\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=list(values)))\n",
    "\n",
    "with open(DATASET_PATH + 'shuffled_since_january.csv', 'r', newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "    for idx, article in enumerate(reader):\n",
    "        if idx % 100 == 0:\n",
    "            print(\"Writing example\", idx, \"of 553030 : \", str(round(idx/553030 * 100, 2)) + \"%\")\n",
    "            elapsed = time() - t1\n",
    "            print('Elapsed time is %f seconds.' % elapsed)\n",
    "        if idx > 1000:\n",
    "            break\n",
    "        if len(article) < 1:\n",
    "            continue\n",
    "\n",
    "        input_tokens = tokenizer.encode_plus(\n",
    "            article[0],\n",
    "            max_length=SEQUENCE_LENGTH,\n",
    "            pad_to_max_length=SEQUENCE_LENGTH,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors='tf',\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "        )\n",
    "\n",
    "        inputs = {\n",
    "            \"attention_mask\": input_tokens[\"attention_mask\"],\n",
    "            \"token_type_ids\": input_tokens[\"token_type_ids\"],\n",
    "            \"training\": False\n",
    "        }\n",
    "\n",
    "        transformer_outputs = models[\"categorisation\"].transformer(input_tokens[\"input_ids\"], **inputs)\n",
    "        output = transformer_outputs[0]\n",
    "        \n",
    "        record_feature = collections.OrderedDict()\n",
    "\n",
    "        ner_predictions = models[\"ner\"](input_tokens[\"input_ids\"], **inputs)\n",
    "        pos_predictions = models[\"pos\"](input_tokens[\"input_ids\"], **inputs)\n",
    "        cat_predictions = models[\"categorisation\"](input_tokens[\"input_ids\"], **inputs)\n",
    "\n",
    "        record_feature[\"cls_token\"] = create_float_feature(output[0][0])\n",
    "        record_feature[\"ner\"] = create_int_feature(np.argmax(ner_predictions[0], axis=2)[0])\n",
    "        record_feature[\"pos\"] = create_int_feature(np.argmax(pos_predictions[0], axis=2)[0])\n",
    "        record_feature[\"categorisation\"] = create_float_feature(np.round(cat_predictions[0])[0])\n",
    "        record_feature[\"label_idx\"] = create_int_feature([idx])\n",
    "\n",
    "        tf_example = tf.train.Example(features=tf.train.Features(feature=record_feature))\n",
    "        writer.write(tf_example.SerializeToString())\n",
    "            \n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
