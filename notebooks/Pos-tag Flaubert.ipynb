{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../flaubert_token_classification.py\n",
    "%run ../tools/utils_ner.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import math\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from seqeval import metrics\n",
    "from fastprogress import master_bar, progress_bar\n",
    "from transformers import (\n",
    "    TF2_WEIGHTS_NAME,\n",
    "    FlaubertConfig,\n",
    "    FlaubertTokenizer,\n",
    "    GradientAccumulator,\n",
    "    create_optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_FOLDER = os.path.abspath(os.path.join(os.getcwd(), os.pardir)) + \"/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = ROOT_FOLDER + \"models/dev/pos/\"\n",
    "LOG_PATH = ROOT_FOLDER + \"logs/pos/\"\n",
    "\n",
    "model_name=\"jplu/tf-flaubert-base-cased\"\n",
    "\n",
    "max_seq_length=64\n",
    "batch_size=32\n",
    "epochs=3\n",
    "learning_rate=5e-5\n",
    "max_grad_norm=1.0\n",
    "warmup_steps=0\n",
    "pad_token_label_id = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = ROOT_FOLDER + \"dataset/pos_tag/UD_French-GSD/\"\n",
    "LABEL_PATH = DATASET_PATH + \"labels.txt\"\n",
    "\n",
    "labels, num_labels = get_labels(LABEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_steps=1500\n",
    "logging_steps=100\n",
    "seed=42\n",
    "no_cuda=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(strategy, train_dataset, tokenizer, model, num_train_examples, labels, pad_token_label_id):\n",
    "    num_train_steps = math.ceil(num_train_examples / batch_size) // epochs\n",
    "\n",
    "    writer = tf.summary.create_file_writer(\"/tmp/mylogs\")\n",
    "\n",
    "    with strategy.scope():\n",
    "        loss_fct = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)\n",
    "        optimizer = create_optimizer(learning_rate, num_train_steps, warmup_steps)\n",
    "        loss_metric = tf.keras.metrics.Mean(name=\"loss\", dtype=tf.float32)\n",
    "        gradient_accumulator = GradientAccumulator()\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    @tf.function\n",
    "    def apply_gradients():\n",
    "        grads_and_vars = []\n",
    "\n",
    "        for gradient, variable in zip(gradient_accumulator.gradients, model.trainable_variables):\n",
    "            if gradient is not None:\n",
    "                grads_and_vars.append((gradient, variable))\n",
    "            else:\n",
    "                grads_and_vars.append((gradient, variable))\n",
    "\n",
    "        optimizer.apply_gradients(grads_and_vars, max_grad_norm)\n",
    "        gradient_accumulator.reset()\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(train_features, train_labels):\n",
    "        def step_fn(train_features, train_labels):\n",
    "            inputs = {\n",
    "                \"attention_mask\": train_features[\"input_mask\"],\n",
    "                \"token_type_ids\": train_features[\"segment_ids\"],\n",
    "                \"training\": True\n",
    "            }\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                logits = model(train_features[\"input_ids\"], **inputs)[0]\n",
    "                active_loss = tf.reshape(train_labels, (-1,)) != pad_token_label_id\n",
    "                active_logits = tf.boolean_mask(tf.reshape(logits, (-1, len(labels))), active_loss)\n",
    "                active_labels = tf.boolean_mask(tf.reshape(train_labels, (-1,)), active_loss)\n",
    "                cross_entropy = loss_fct(active_labels, active_logits)\n",
    "                loss = tf.reduce_sum(cross_entropy) * (1.0 / batch_size)\n",
    "                grads = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "                gradient_accumulator(grads)\n",
    "\n",
    "            return cross_entropy\n",
    "\n",
    "        per_example_losses = strategy.experimental_run_v2(step_fn, args=(train_features, train_labels))\n",
    "        mean_loss = strategy.reduce(tf.distribute.ReduceOp.MEAN, per_example_losses, axis=0)\n",
    "\n",
    "        return mean_loss\n",
    "\n",
    "    current_time = datetime.datetime.now()\n",
    "    train_iterator = master_bar(range(epochs))\n",
    "    global_step = 0\n",
    "    logging_loss = 0.0\n",
    "\n",
    "    for epoch in train_iterator:\n",
    "        epoch_iterator = progress_bar(\n",
    "            train_dataset, total=num_train_steps, parent=train_iterator, display=False\n",
    "        )\n",
    "\n",
    "        step = 1\n",
    "\n",
    "        with strategy.scope():\n",
    "            for train_features, train_labels in epoch_iterator:\n",
    "                loss = train_step(train_features, train_labels)\n",
    "                strategy.experimental_run_v2(apply_gradients)\n",
    "                loss_metric(loss)\n",
    "                global_step += 1\n",
    "\n",
    "                # Log metrics\n",
    "                if logging_steps > 0 and global_step % logging_steps == 0:\n",
    "                    lr = optimizer.learning_rate\n",
    "                    learning_rate_step = lr(step)\n",
    "\n",
    "                    with writer.as_default():\n",
    "                        tf.summary.scalar(\"lr\", learning_rate_step, global_step)\n",
    "                        tf.summary.scalar(\"loss\", (loss_metric.result() - logging_loss) / logging_steps, global_step)\n",
    "\n",
    "                    logging_loss = loss_metric.result()\n",
    "\n",
    "                with writer.as_default():\n",
    "                    tf.summary.scalar(\"loss\", loss_metric.result(), step=step)\n",
    "\n",
    "                # Save model checkpoint\n",
    "                if save_steps > 0 and global_step % save_steps == 0:\n",
    "                    checkpoint_output_dir = os.path.join(MODEL_PATH, \"checkpoint-{}\".format(global_step))\n",
    "\n",
    "                    if not os.path.exists(checkpoint_output_dir):\n",
    "                        os.makedirs(checkpoint_output_dir)\n",
    "\n",
    "                    model.save_pretrained(checkpoint_output_dir)\n",
    "                    print(\"Saving model checkpoint to\", checkpoint_output_dir)\n",
    "\n",
    "                train_iterator.child.comment = f\"loss : {loss_metric.result()}\"\n",
    "                step += 1\n",
    "\n",
    "        train_iterator.write(f\"loss epoch {epoch + 1}: {loss_metric.result()}\")\n",
    "        loss_metric.reset_states()\n",
    "\n",
    "    print(\"  Training took time = {}\".format(datetime.datetime.now() - current_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(strategy, model, tokenizer, labels, pad_token_label_id, mode):\n",
    "    eval_dataset, size = load_and_cache_examples(tokenizer, labels, pad_token_label_id, mode=mode)\n",
    "    eval_dataset = strategy.experimental_distribute_dataset(eval_dataset)\n",
    "    preds = None\n",
    "    \n",
    "    num_eval_steps = math.ceil(size / batch_size)\n",
    "    loss_fct = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)\n",
    "    loss = 0.0\n",
    "\n",
    "    for eval_features, eval_labels in eval_dataset:\n",
    "        inputs = {\n",
    "            \"attention_mask\": eval_features[\"input_mask\"],\n",
    "            \"token_type_ids\": eval_features[\"segment_ids\"],\n",
    "            \"training\": False\n",
    "        }\n",
    "\n",
    "        with strategy.scope():\n",
    "            logits = model(eval_features[\"input_ids\"], **inputs)[0]\n",
    "            active_loss = tf.reshape(eval_labels, (-1,)) != pad_token_label_id\n",
    "            active_logits = tf.boolean_mask(tf.reshape(logits, (-1, len(labels))), active_loss)\n",
    "            active_labels = tf.boolean_mask(tf.reshape(eval_labels, (-1,)), active_loss)\n",
    "            cross_entropy = loss_fct(active_labels, active_logits)\n",
    "            loss += tf.reduce_sum(cross_entropy) * (1.0 / batch_size)\n",
    "\n",
    "        if preds is None:\n",
    "            preds = logits.numpy()\n",
    "            label_ids = eval_labels.numpy()\n",
    "        else:\n",
    "            preds = np.append(preds, logits.numpy(), axis=0)\n",
    "            label_ids = np.append(label_ids, eval_labels.numpy(), axis=0)\n",
    "\n",
    "    preds = np.argmax(preds, axis=2)\n",
    "    y_pred = [[] for _ in range(label_ids.shape[0])]\n",
    "    y_true = [[] for _ in range(label_ids.shape[0])]\n",
    "    loss = loss / num_eval_steps\n",
    "\n",
    "    for i in range(label_ids.shape[0]):\n",
    "        for j in range(label_ids.shape[1]):\n",
    "            if label_ids[i, j] != pad_token_label_id:\n",
    "                y_pred[i].append(labels[preds[i, j] - 1])\n",
    "                y_true[i].append(labels[label_ids[i, j] - 1])\n",
    "\n",
    "    return y_true, y_pred, loss.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_cache_examples(tokenizer, labels, pad_token_label_id, mode):\n",
    "    # Load data features from cache or dataset file\n",
    "    cached_features_file = os.path.join(\n",
    "        DATASET_PATH,\n",
    "        \"cached_{}_{}_{}.tf_record\".format(\n",
    "            mode, list(filter(None, model_name.split(\"/\"))).pop(), str(max_seq_length)\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    print(\"Creating features from dataset file at\", DATASET_PATH)\n",
    "    examples = read_examples_from_file(DATASET_PATH, mode)\n",
    "    features = convert_examples_to_features(\n",
    "        examples,\n",
    "        labels,\n",
    "        max_seq_length,\n",
    "        tokenizer,\n",
    "        cls_token_at_end=False,\n",
    "        # xlnet has a cls token at the end\n",
    "        cls_token=tokenizer.cls_token,\n",
    "        cls_token_segment_id=0,\n",
    "        sep_token=tokenizer.sep_token,\n",
    "        sep_token_extra=False, # roberta uses an extra separator b/w pairs of sentences, cf. github.com/pytorch/fairseq/commit/1684e166e3da03f5b600dbb7855cb98ddfcd0805\n",
    "        pad_on_left=False, # pad on the left for xlnet\n",
    "        pad_token=tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n",
    "        pad_token_segment_id=0,\n",
    "        pad_token_label_id=pad_token_label_id,\n",
    "    )\n",
    "\n",
    "    print(\"#\", len(features) ,\" lines in dataset \")\n",
    "    save_cache(features, cached_features_file)\n",
    "    dataset, size = load_cache(cached_features_file, max_seq_length)\n",
    "\n",
    "    if mode == \"train\":\n",
    "        dataset = dataset.repeat()\n",
    "        dataset = dataset.shuffle(buffer_size=8192, seed=seed)\n",
    "\n",
    "    dataset = dataset.batch(batch_size, True)\n",
    "    dataset = dataset.prefetch(buffer_size=batch_size)\n",
    "\n",
    "    return dataset, size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\" if no_cuda else \"/gpu:0\")\n",
    "\n",
    "config = FlaubertConfig.from_pretrained(model_name, num_labels=num_labels)\n",
    "\n",
    "with strategy.scope():\n",
    "    model = TFFlaubertForTokenClassification.from_pretrained(model_name, config=config)\n",
    "    tokenizer = FlaubertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating features from dataset file at /home/jupyter/bert_clustering/dataset/pos_tag/UD_French-GSD/\n",
      "# 13361  lines in dataset \n"
     ]
    }
   ],
   "source": [
    "train_dataset, num_train_examples = load_and_cache_examples(tokenizer, labels, pad_token_label_id, mode=\"train\")\n",
    "train_dataset = strategy.experimental_distribute_dataset(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_flaubert_for_token_classification\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dropout_26 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  14611     \n",
      "_________________________________________________________________\n",
      "transformer (TFFlaubertMainL multiple                  138233088 \n",
      "=================================================================\n",
      "Total params: 138,247,699\n",
      "Trainable params: 138,247,699\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "loss epoch 1: 0.5947258472442627<p>loss epoch 2: 0.10725193470716476<p>loss epoch 3: 0.10697513818740845"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyter/.local/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyter/.local/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "/home/jupyter/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/home/jupyter/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/home/jupyter/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training took time = 0:03:39.105156\n",
      "Saving model to /home/jupyter/bert_clustering/models/dev/pos/\n"
     ]
    }
   ],
   "source": [
    "train(strategy, train_dataset, tokenizer, model, num_train_examples, labels, pad_token_label_id)\n",
    "\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    os.makedirs(MODEL_PATH)\n",
    "\n",
    "print(\"Saving model to\", MODEL_PATH)\n",
    "\n",
    "model.save_pretrained(MODEL_PATH)\n",
    "tokenizer.save_pretrained(MODEL_PATH)\n",
    "\n",
    "del train_dataset, num_train_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating features from dataset file at /home/jupyter/bert_clustering/dataset/pos_tag/UD_French-GSD/\n",
      "# 1375  lines in dataset \n",
      "final_loss = 41.62265\n",
      "final_report\n",
      "\n",
      "           precision    recall  f1-score   support\n",
      "\n",
      "    CCONJ     0.9957    0.9951    0.9954      5305\n",
      "      ADJ     0.9934    0.9940    0.9937      5486\n",
      "    PROPN     0.9959    0.9986    0.9972      2174\n",
      "     INTJ     0.9700    0.9620    0.9660      6376\n",
      "      SYM     0.9662    0.9745    0.9704      2670\n",
      "      ADP     0.9290    0.9389    0.9339      1212\n",
      "      AUX     0.9788    0.9835    0.9812       847\n",
      "     PRON     0.8962    0.8998    0.8980      1957\n",
      "     PART     0.9823    0.9554    0.9687      1278\n",
      "      ADV     0.9741    0.9816    0.9778      1033\n",
      "        X     0.9951    0.9951    0.9951      1013\n",
      "     NOUN     0.9400    0.9503    0.9451       906\n",
      "    SCONJ     0.8718    0.5862    0.7010        58\n",
      "    PUNCT     0.9447    0.9328    0.9387       238\n",
      "      DET     0.0000    0.0000    0.0000         5\n",
      "     VERB     0.3000    0.2353    0.2637        51\n",
      "      NUM     0.0000    0.0000    0.0000         8\n",
      "\n",
      "micro avg     0.9734    0.9715    0.9724     30617\n",
      "macro avg     0.9727    0.9715    0.9720     30617\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true, y_pred, eval_loss = evaluate(strategy, model, tokenizer, labels, pad_token_label_id, mode=\"dev\")\n",
    "report = metrics.classification_report(y_true, y_pred, digits=4)\n",
    "\n",
    "output_eval_file = os.path.join(MODEL_PATH, \"eval_results.txt\")\n",
    "\n",
    "with tf.io.gfile.GFile(output_eval_file, \"w\") as writer:\n",
    "    # Log loss\n",
    "    print(\"final_loss = \" + str(eval_loss))\n",
    "    writer.write(\"final_loss = \" + str(eval_loss))\n",
    "    writer.write(\"\\n\")\n",
    "\n",
    "    # Log metrics\n",
    "    print(\"final_report\")\n",
    "    print(\"\\n\" + report)\n",
    "    writer.write(\"final_report\" + \"\\n\")\n",
    "    writer.write(report)\n",
    "    writer.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = FlaubertTokenizer.from_pretrained(MODEL_PATH)\n",
    "# model = TFFlaubertForTokenClassification.from_pretrained(MODEL_PATH)\n",
    "\n",
    "# # predict_dataset, _ = load_and_cache_examples(tokenizer, labels, pad_token_label_id, mode=\"test\")\n",
    "\n",
    "# y_true, y_pred, pred_loss = evaluate(strategy, model, tokenizer, labels, pad_token_label_id, mode=\"test\")\n",
    "# output_test_results_file = os.path.join(MODEL_PATH, \"test_results.txt\")\n",
    "# output_test_predictions_file = os.path.join(MODEL_PATH, \"test_predictions.txt\")\n",
    "# report = metrics.classification_report(y_true, y_pred, digits=4)\n",
    "\n",
    "# with tf.io.gfile.GFile(output_test_results_file, \"w\") as writer:\n",
    "#     report = metrics.classification_report(y_true, y_pred, digits=4)\n",
    "\n",
    "#     logging.info(\"\\n\" + report)\n",
    "\n",
    "#     writer.write(report)\n",
    "#     writer.write(\"\\n\\nloss = \" + str(pred_loss))\n",
    "\n",
    "# with tf.io.gfile.GFile(output_test_predictions_file, \"w\") as writer:\n",
    "#     with tf.io.gfile.GFile(os.path.join(DATASET_PATH, \"test.txt\"), \"r\") as f:\n",
    "#         example_id = 0\n",
    "\n",
    "#         for line in f:\n",
    "#             if line.startswith(\"-DOCSTART-\") or line == \"\" or line == \"\\n\":\n",
    "#                 writer.write(line)\n",
    "\n",
    "#                 if not y_pred[example_id]:\n",
    "#                     example_id += 1\n",
    "#             elif y_pred[example_id]:\n",
    "#                 output_line = line.split()[0] + \" \" + y_pred[example_id].pop(0) + \"\\n\"\n",
    "#                 writer.write(output_line)\n",
    "#             else:\n",
    "#                 logging.warning(\"Maximum sequence length exceeded: No prediction for '%s'.\", line.split()[0])\n"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
