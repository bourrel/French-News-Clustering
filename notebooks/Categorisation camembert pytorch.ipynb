{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HBGDeeP7atmL"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, LabelBinarizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils import class_weight\n",
    "from transformers import (\n",
    "    CamembertTokenizer,\n",
    "    CamembertForSequenceClassification\n",
    ")\n",
    "from transformers.modeling_tf_utils import get_initializer\n",
    "from tensorflow.keras.layers import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qyjNf9r3atmX"
   },
   "source": [
    "## Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot encoder class label by alphabetical order\n",
    "labels = ['santé', 'science_high-tech', 'sports', 'économie'] #'international', 'culture', 'france', ]# 'homepage', \n",
    "#{'sports': 1.0, 'économie': 3.116034374345001, 'santé': 17.25464252553389, 'science_high-tech': 3.7884409561184444}\n",
    "\n",
    "class_weights = {\n",
    "    0: 1.0,\n",
    "    1: 1.0959043500381582,\n",
    "    2: 1.0959043500381582,\n",
    "    3: 1.0788880540946657,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = LabelBinarizer()\n",
    "#enc.fit(labels)\n",
    "enc.fit(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(enc.transform([\"santé\"]))\n",
    "print(enc.transform([\"science_high-tech\"]))\n",
    "print(enc.transform([\"sports\"]))\n",
    "print(enc.transform([\"économie\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(['au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle', 'en', 'et', 'eux', 'il', 'ils', 'je', 'la', 'le', 'les', 'leur', 'lui', 'ma', 'mais', 'me', 'même', 'mes', 'moi', 'mon', 'ne', 'nos', 'notre', 'nous', 'on', 'ou', 'par', 'pas', 'pour', 'qu', 'que', 'qui', 'sa', 'se', 'ses', 'son', 'sur', 'ta', 'te', 'tes', 'toi', 'ton', 'tu', 'un', 'une', 'vos', 'votre', 'vous', 'c', 'd', 'j', 'l', 'à', 'm', 'n', 's', 't', 'y', 'été', 'étée', 'étées', 'étés', 'étant', 'étante', 'étants', 'étantes', 'suis', 'es', 'est', 'sommes', 'êtes', 'sont', 'serai', 'seras', 'sera', 'serons', 'serez', 'seront', 'serais', 'serait', 'serions', 'seriez', 'seraient', 'étais', 'était', 'étions', 'étiez', 'étaient', 'fus', 'fut', 'fûmes', 'fûtes', 'furent', 'sois', 'soit', 'soyons', 'soyez', 'soient', 'fusse', 'fusses', 'fût', 'fussions', 'fussiez', 'fussent', 'ayant', 'ayante', 'ayantes', 'ayants', 'eu', 'eue', 'eues', 'eus', 'ai', 'as', 'avons', 'avez', 'ont', 'aurai', 'auras', 'aura', 'aurons', 'aurez', 'auront', 'aurais', 'aurait', 'aurions', 'auriez', 'auraient', 'avais', 'avait', 'avions', 'aviez', 'avaient', 'eut', 'eûmes', 'eûtes', 'eurent', 'aie', 'aies', 'ait', 'ayons', 'ayez', 'aient', 'eusse', 'eusses', 'eût', 'eussions', 'eussiez', 'eussent'])\n",
    "\n",
    "whitespace = re.compile(\"[\\\\s]+\", re.UNICODE)\n",
    "dash = re.compile(\"[\\\\-\\\\˗\\\\֊\\\\‐\\\\‑\\\\‒\\\\–\\\\—\\\\⁻\\\\₋\\\\−\\\\﹣\\\\－]\")\n",
    "left_parenthesis_filter = re.compile(\"[\\\\(\\\\[\\\\{\\\\⁽\\\\₍\\\\❨\\\\❪\\\\﹙\\\\（]\")\n",
    "right_parenthesis_filter = re.compile(\"[\\\\)\\\\]\\\\}\\\\⁾\\\\₎\\\\❩\\\\❫\\\\﹚\\\\）]\")\n",
    "currencies = re.compile(\"[¥£₪$€฿₨]\")\n",
    "apostrophe_filter = re.compile(\n",
    "    r'&#39;|[ʼ՚＇‘’‛❛❜ߴߵ`‵´ˊˋ{}{}{}{}{}{}{}{}{}]'.format(\n",
    "        chr(768), chr(769), chr(832),\n",
    "        chr(833), chr(2387), chr(5151),\n",
    "        chr(5152), chr(65344), chr(8242)\n",
    "    ), re.UNICODE\n",
    ")\n",
    "basic_cleaner = re.compile(r'[^\\w\\s{}]'.format(re.escape(\"€-!?/;\\\"'%&<>.()@#:,|=*\")), re.UNICODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_main_category(dictOfNames):\n",
    "    new_dict = {}\n",
    "    try:\n",
    "        for (key,value) in dictOfNames.items():\n",
    "            #if \"score\" in key or \"applenews\" in key or \"homepage\" in key:\n",
    "            #    continue\n",
    "            new_key = re.sub(r'desktop_|mobile_webview_', \"\", key)\n",
    "            new_key = re.sub(r'google_', \"\", new_key)\n",
    "            if new_key not in labels:\n",
    "                continue\n",
    "            if new_key not in new_dict:\n",
    "                new_dict[new_key] = 0\n",
    "            new_dict[new_key] += value\n",
    "        #return [key for key in new_dict.keys()]\n",
    "        return max(new_dict, key=new_dict.get)\n",
    "    except ValueError as e :\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = str.strip(str.lower(text))\n",
    "    text = whitespace.sub(' ', text)\n",
    "    text = dash.sub('-', text)\n",
    "    text = currencies.sub('€', text)\n",
    "    text = apostrophe_filter.sub(\"'\", text)\n",
    "    text = left_parenthesis_filter.sub(\"(\", text)\n",
    "    text = right_parenthesis_filter.sub(\")\", text)\n",
    "    text = basic_cleaner.sub('', text)\n",
    "    #return text\n",
    "    result = []\n",
    "    for word in re.split(\"\\W+\", text):\n",
    "      if word not in stopwords:\n",
    "        result.append(word)\n",
    "    return \" \".join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lines = open('since_january.csv').readlines()\n",
    "#lines = lines[1:]\n",
    "#random.shuffle(lines)\n",
    "#print(\"# lines : \", len(lines))\n",
    "#open('shuffled_since_january.csv', 'w').writelines(lines)\n",
    "#del lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_generator():\n",
    "    samples = []\n",
    "    categories = []\n",
    "    idx = 0\n",
    "    while 1:\n",
    "        with open('shuffled_since_january.csv', 'r', newline='') as csvfile:\n",
    "            reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "            idx = 0\n",
    "            for i, row in enumerate(reader):\n",
    "                if len(row) < 3 or row[3] not in labels:\n",
    "                    continue\n",
    "\n",
    "                text = row[0]\n",
    "                category = \"\"\n",
    "                if row[4] != {} and row[4] != \"\":\n",
    "                    category = get_main_category(json.loads(row[4]))\n",
    "                if category == \"\":\n",
    "                    continue\n",
    "\n",
    "                #{'sports': 74333, 'économie': 23855, 'santé': 4308, 'science_high-tech': 19621}\n",
    "                if category == 'sports' and i % 19 != 0:\n",
    "                    continue\n",
    "                elif category == 'économie' and i % 6 != 0:\n",
    "                    continue\n",
    "                elif category == 'science_high-tech' and i % 5 != 0:\n",
    "                    continue\n",
    "\n",
    "                samples.append(tokenizer.encode(clean_text(text), pad_to_max_length=32, add_special_tokens=True))\n",
    "                categories.append(category)\n",
    "\n",
    "                idx += 1\n",
    "                if idx >= BATCH_SIZE:\n",
    "                    categories = enc.transform(categories)\n",
    "                    yield torch.tensor(samples).cuda(), torch.tensor(categories).cuda()\n",
    "                    samples = []\n",
    "                    categories = []\n",
    "                    idx = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ki-ztvyKatmQ"
   },
   "source": [
    "## Import camembert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jPFITcsIatmR"
   },
   "outputs": [],
   "source": [
    "model = CamembertForSequenceClassification.from_pretrained(\n",
    "    \"camembert-base\",\n",
    "    num_labels=len(labels),\n",
    "    #force_download=True\n",
    ")\n",
    "tokenizer = CamembertTokenizer.from_pretrained(\n",
    "    \"camembert-base\",\n",
    "    output_hidden_states=True,\n",
    "    output_attentions=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (a, b) in enumerate(file_generator()):\n",
    "    if idx > 1:\n",
    "        break\n",
    "    #print(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "aHDtNl43atmU",
    "outputId": "e9b3c96e-b085-450f-d733-b73bcb6a14db"
   },
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(tokenizer.encode(\"Sida. Une start-up française découvre une avancée majeure dans la lutte contre le VIH\", add_special_tokens=True)).unsqueeze(0)\n",
    "out = model(input_ids)\n",
    "\n",
    "print(out)\n",
    "#print(np.argmax(out[0]) - 1)\n",
    "#print(list(labels)[np.argmax(out[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "agGTsSo9atmm"
   },
   "source": [
    "## Train model on new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "model.cuda()\n",
    "\n",
    "#Define loss criterion\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#Define the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "losses = []\n",
    "for idx, (x_batch, y_batch) in enumerate(file_generator()):\n",
    "    for x, y in zip(x_batch, y_batch):\n",
    "        print(x, y)\n",
    "        #if idx % 10 == 0: # Validation\n",
    "        #    a = model(x)[0].detach().cpu()\n",
    "        #    print(a[:2])\n",
    "        #    #a = np.argmax(a, decimals=-1)\n",
    "        #    print(accuracy_score(np.argmax(a, axis=1), np.argmax(y.cpu(), axis=1)))\n",
    "        #else:\n",
    "        #Precit the output for Given input\n",
    "        print(torch.Tensor([x]))\n",
    "        y_pred = 0#model.forward(cuda())\n",
    "        #Compute Cross entropy loss\n",
    "        loss = criterion(y_pred, torch.max(y, 1)[1])\n",
    "        #Add loss to the list\n",
    "        losses.append(loss.item())\n",
    "        #Clear the previous gradients\n",
    "        optimizer.zero_grad()\n",
    "        #Compute gradients\n",
    "        loss.backward()\n",
    "        #Adjust weights\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yJIId2T-s8QM"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.layers[0])\n",
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VHke8uQhatms"
   },
   "outputs": [],
   "source": [
    "input_ids = tf.constant(\n",
    "    [\n",
    "        #tokenizer.encode(\n",
    "        #    clean_text(\"Sida. Une start-up française découvre une avancée majeure dans la lutte contre le VIH\"),\n",
    "        #    add_special_tokens=True\n",
    "        #),\n",
    "        tokenizer.encode(\n",
    "            clean_text(\"Annuler l’Euro 2020 ferait perdre très gros à l’UEFA\"),\n",
    "            add_special_tokens=True\n",
    "        )\n",
    "    ], tf.int32\n",
    ")\n",
    "out = model(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pNqWse-4emRO"
   },
   "outputs": [],
   "source": [
    "print(out)\n",
    "print(np.argmax(out[0]))\n",
    "print(list(labels)[np.argmax(out[0])])\n",
    "\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "c, d = next(file_generator())\n",
    "\n",
    "#Confution Matrix and Classification Report\n",
    "Y_pred = model(c)\n",
    "#print(Y_pred)\n",
    "y_pred = [labels[int(np.argmax(y))] for y in Y_pred[0]]\n",
    "d = enc.inverse_transform(d.numpy())\n",
    "\n",
    "for x, y in zip(d, y_pred):\n",
    "    print(x, \"/\", y)\n",
    "\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(d, y_pred))\n",
    "print('Classification Report')\n",
    "\n",
    "print(classification_report(d, y_pred, target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Import && train model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
