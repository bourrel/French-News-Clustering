{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HBGDeeP7atmL"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, MultiLabelBinarizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils import class_weight\n",
    "from transformers import (\n",
    "    CamembertTokenizer,\n",
    "    TFCamembertModel,\n",
    "    TFRobertaPreTrainedModel,\n",
    "    TFRobertaMainLayer\n",
    ")\n",
    "from transformers.modeling_tf_utils import get_initializer\n",
    "from tensorflow.keras.layers import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "STEPS = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qyjNf9r3atmX"
   },
   "source": [
    "## Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot encoder class label by alphabetical order\n",
    "labels = ['santé', 'science_high-tech', 'sports', 'économie'] #'international', 'culture', 'france', ]# 'homepage', \n",
    "class_weights = [\n",
    "    {0: 14.5},#6.06},\n",
    "    {1: 3.5},#1.47},\n",
    "    {2: 1.0},#0.42},\n",
    "    {3: 3.16}#1.32 }\n",
    "]\n",
    "\n",
    "#class_weights = {\n",
    "#    0: 1.0,\n",
    "#    1: 2.0712994058382845,\n",
    "#    2: 2.026282537275714,\n",
    "#    3: 1.5499710032862941,\n",
    "#    4: 3.541910546659304,\n",
    "#    5: 14.931098696461826,\n",
    "#    6: 17.023354564755838,\n",
    "#    7: 3.301966436734274\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categories='auto', drop=None, dtype=<class 'numpy.float64'>,\n",
       "              handle_unknown='error', sparse=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = OneHotEncoder()\n",
    "#enc.fit(labels)\n",
    "enc.fit([[label] for label in labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0.]]\n",
      "[[0. 1. 0. 0.]]\n",
      "[[0. 0. 1. 0.]]\n",
      "[[0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(enc.transform([[\"santé\"]]).toarray())\n",
    "print(enc.transform([[\"science_high-tech\"]]).toarray())\n",
    "print(enc.transform([[\"sports\"]]).toarray())\n",
    "print(enc.transform([[\"économie\"]]).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(['au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle', 'en', 'et', 'eux', 'il', 'ils', 'je', 'la', 'le', 'les', 'leur', 'lui', 'ma', 'mais', 'me', 'même', 'mes', 'moi', 'mon', 'ne', 'nos', 'notre', 'nous', 'on', 'ou', 'par', 'pas', 'pour', 'qu', 'que', 'qui', 'sa', 'se', 'ses', 'son', 'sur', 'ta', 'te', 'tes', 'toi', 'ton', 'tu', 'un', 'une', 'vos', 'votre', 'vous', 'c', 'd', 'j', 'l', 'à', 'm', 'n', 's', 't', 'y', 'été', 'étée', 'étées', 'étés', 'étant', 'étante', 'étants', 'étantes', 'suis', 'es', 'est', 'sommes', 'êtes', 'sont', 'serai', 'seras', 'sera', 'serons', 'serez', 'seront', 'serais', 'serait', 'serions', 'seriez', 'seraient', 'étais', 'était', 'étions', 'étiez', 'étaient', 'fus', 'fut', 'fûmes', 'fûtes', 'furent', 'sois', 'soit', 'soyons', 'soyez', 'soient', 'fusse', 'fusses', 'fût', 'fussions', 'fussiez', 'fussent', 'ayant', 'ayante', 'ayantes', 'ayants', 'eu', 'eue', 'eues', 'eus', 'ai', 'as', 'avons', 'avez', 'ont', 'aurai', 'auras', 'aura', 'aurons', 'aurez', 'auront', 'aurais', 'aurait', 'aurions', 'auriez', 'auraient', 'avais', 'avait', 'avions', 'aviez', 'avaient', 'eut', 'eûmes', 'eûtes', 'eurent', 'aie', 'aies', 'ait', 'ayons', 'ayez', 'aient', 'eusse', 'eusses', 'eût', 'eussions', 'eussiez', 'eussent'])\n",
    "\n",
    "whitespace = re.compile(\"[\\\\s]+\", re.UNICODE)\n",
    "dash = re.compile(\"[\\\\-\\\\˗\\\\֊\\\\‐\\\\‑\\\\‒\\\\–\\\\—\\\\⁻\\\\₋\\\\−\\\\﹣\\\\－]\")\n",
    "left_parenthesis_filter = re.compile(\"[\\\\(\\\\[\\\\{\\\\⁽\\\\₍\\\\❨\\\\❪\\\\﹙\\\\（]\")\n",
    "right_parenthesis_filter = re.compile(\"[\\\\)\\\\]\\\\}\\\\⁾\\\\₎\\\\❩\\\\❫\\\\﹚\\\\）]\")\n",
    "currencies = re.compile(\"[¥£₪$€฿₨]\")\n",
    "apostrophe_filter = re.compile(\n",
    "    r'&#39;|[ʼ՚＇‘’‛❛❜ߴߵ`‵´ˊˋ{}{}{}{}{}{}{}{}{}]'.format(\n",
    "        chr(768), chr(769), chr(832),\n",
    "        chr(833), chr(2387), chr(5151),\n",
    "        chr(5152), chr(65344), chr(8242)\n",
    "    ), re.UNICODE\n",
    ")\n",
    "basic_cleaner = re.compile(r'[^\\w\\s{}]'.format(re.escape(\"€-!?/;\\\"'%&<>.()@#:,|=*\")), re.UNICODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_main_category(dictOfNames):\n",
    "    new_dict = {}\n",
    "    try:\n",
    "        for (key,value) in dictOfNames.items():\n",
    "            #if \"score\" in key or \"applenews\" in key or \"homepage\" in key:\n",
    "            #    continue\n",
    "            new_key = re.sub(r'desktop_|mobile_webview_', \"\", key)\n",
    "            new_key = re.sub(r'google_', \"\", new_key)\n",
    "            if new_key not in labels:\n",
    "                continue\n",
    "            if new_key not in new_dict:\n",
    "                new_dict[new_key] = 0\n",
    "            new_dict[new_key] += value\n",
    "        #return [key for key in new_dict.keys()]\n",
    "        return max(new_dict, key=new_dict.get)\n",
    "    except ValueError as e :\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = str.strip(str.lower(text))\n",
    "    text = whitespace.sub(' ', text)\n",
    "    text = dash.sub('-', text)\n",
    "    text = currencies.sub('€', text)\n",
    "    text = apostrophe_filter.sub(\"'\", text)\n",
    "    text = left_parenthesis_filter.sub(\"(\", text)\n",
    "    text = right_parenthesis_filter.sub(\")\", text)\n",
    "    text = basic_cleaner.sub('', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# lines :  553030\n"
     ]
    }
   ],
   "source": [
    "lines = open('since_january.csv').readlines()\n",
    "lines = lines[1:]\n",
    "random.shuffle(lines)\n",
    "print(\"# lines : \", len(lines))\n",
    "open('shuffled_since_january.csv', 'w').writelines(lines)\n",
    "del lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_generator(steps=1):\n",
    "    samples = []\n",
    "    categories = []\n",
    "    idx = 0\n",
    "    with open('shuffled_since_january.csv', 'r', newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "        idx = 0\n",
    "        for row in reader:\n",
    "            if len(row) < 3 or row[3] not in labels:\n",
    "                continue\n",
    "\n",
    "            idx += 1\n",
    "            text = row[0]\n",
    "            category = \"\"\n",
    "            if row[4] != {} and row[4] != \"\":\n",
    "                category = get_main_category(json.loads(row[4]))\n",
    "            if category == \"\":\n",
    "                continue\n",
    "\n",
    "            samples.append(tokenizer.encode(clean_text(text), pad_to_max_length=16, add_special_tokens=False))\n",
    "            categories.append([category])\n",
    "\n",
    "            if idx >= BATCH_SIZE * steps:\n",
    "                categories = enc.transform(categories).toarray()\n",
    "                #print(categories)\n",
    "                #print(len(samples))\n",
    "                yield tf.convert_to_tensor(samples, dtype=tf.int32),  tf.convert_to_tensor(categories, dtype=tf.int32)\n",
    "                samples = []\n",
    "                categories = []\n",
    "                idx = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ki-ztvyKatmQ"
   },
   "source": [
    "## Import camembert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFRobertaClassificationHead(tf.keras.layers.Layer):\n",
    "    \"\"\"Head for sentence-level classification tasks.\"\"\"\n",
    "\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(config, **kwargs)\n",
    "        self.init = get_initializer(config.initializer_range)\n",
    "        self.batch = BatchNormalization()\n",
    "        self.merge = Concatenate(axis=1)\n",
    "        self.pool = MaxPooling1D(pool_size=2)\n",
    "        self.global_pool = GlobalMaxPooling1D()\n",
    "        self.dropout = Dropout(config.hidden_dropout_prob)\n",
    "        self.dense = Dense(config.hidden_size, activation='relu', kernel_initializer=self.init)\n",
    "        self.out_proj = Dense(config.num_labels, kernel_initializer=self.init, activation=\"sigmoid\", name=\"out_proj\")\n",
    "        self.conv = Conv1D(\n",
    "            filters=config.hidden_size,\n",
    "            activation='relu',\n",
    "            kernel_size=1,\n",
    "            input_shape=(config.max_position_embeddings, config.hidden_size),\n",
    "        )\n",
    "        self.conv2 = Conv1D(\n",
    "            filters=config.hidden_size,\n",
    "            activation='relu',\n",
    "            kernel_size=2,\n",
    "        )\n",
    "\n",
    "    def call(self, features, training=False):\n",
    "        x = features[:, 0, :]\n",
    "        x = self.conv(features)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.global_pool(x)\n",
    "        x = self.dense(x)\n",
    "        x = self.dropout(x, training=training)\n",
    "        x = self.dense(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFCamembertForSequenceClassification(TFRobertaPreTrainedModel):\n",
    "    def __init__(self, config, *inputs, **kwargs):\n",
    "        super().__init__(config, *inputs, **kwargs)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.roberta = TFRobertaMainLayer(config, name=\"roberta\")\n",
    "        self.classifier = TFRobertaClassificationHead(config, name=\"classifier\")\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        outputs = self.roberta(inputs, **kwargs)\n",
    "        sequence_output = outputs[0]\n",
    "        logits = self.classifier(sequence_output, training=kwargs.get(\"training\", False))\n",
    "        outputs = (logits,) + outputs[2:]\n",
    "        return outputs  # logits, (hidden_states), (attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jPFITcsIatmR"
   },
   "outputs": [],
   "source": [
    "model = TFCamembertForSequenceClassification.from_pretrained(\n",
    "    \"jplu/tf-camembert-base\",\n",
    "    num_labels=len(labels)\n",
    ")\n",
    "tokenizer = CamembertTokenizer.from_pretrained(\n",
    "    \"jplu/tf-camembert-base\",\n",
    "    output_hidden_states=True,\n",
    "    output_attentions=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#special_tokens_dict = {'cls_token': '<CLS>'}\n",
    "#tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "aHDtNl43atmU",
    "outputId": "e9b3c96e-b085-450f-d733-b73bcb6a14db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[0.50389194, 0.4961208 , 0.50139683, 0.49829933]], dtype=float32)>,)\n",
      "-1\n",
      "santé\n"
     ]
    }
   ],
   "source": [
    "input_ids = tf.constant([tokenizer.encode(\"Sida. Une start-up française découvre une avancée majeure dans la lutte contre le VIH\", add_special_tokens=False, pad_to_max_length=34)], tf.int32)\n",
    "out = model(input_ids)\n",
    "\n",
    "print(out)\n",
    "print(np.argmax(out[0]) - 1)\n",
    "print(list(labels)[np.argmax(out[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "agGTsSo9atmm"
   },
   "source": [
    "## Train model on new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A4oan2z0atmn"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(log_dir=\"./log_january/\"+time.strftime(\"%d%m%y/%H:%M:%S\"))\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss,\n",
    "    metrics=[\"categorical_accuracy\"],\n",
    "    #loss_weights=list(class_weights.values())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yJIId2T-s8QM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_camembert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "roberta (TFRobertaMainLayer) multiple                  110621952 \n",
      "_________________________________________________________________\n",
      "classifier (TFRobertaClassif multiple                  1184260   \n",
      "=================================================================\n",
      "Total params: 111,806,212\n",
      "Trainable params: 111,806,212\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.modeling_tf_roberta.TFRobertaMainLayer object at 0x7ff523e58dd0>\n"
     ]
    }
   ],
   "source": [
    "print(model.layers[0])\n",
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 320 steps\n",
      "Epoch 1/100\n",
      "119/320 [==========>...................] - ETA: 5:33 - loss: 1.2194 - categorical_accuracy: 0.6226"
     ]
    }
   ],
   "source": [
    "#labels_weight = np.array([14.5, 3.5, 1.0, 3.16])\n",
    "#labels_weight = np.array([labels_weight[np.argmax(q)] for q in y])\n",
    "\n",
    "model.fit(\n",
    "    file_generator(),\n",
    "    epochs=100,\n",
    "    class_weight=class_weights,\n",
    "    #sample_weight=labels_weight,\n",
    "    steps_per_epoch=STEPS * 10,\n",
    "    max_queue_size=8, \n",
    "    verbose=1,\n",
    "    #validation_split = 0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VHke8uQhatms"
   },
   "outputs": [],
   "source": [
    "input_ids = tf.constant(\n",
    "    [\n",
    "        #tokenizer.encode(\n",
    "        #    clean_text(\"Sida. Une start-up française découvre une avancée majeure dans la lutte contre le VIH\"),\n",
    "        #    add_special_tokens=True\n",
    "        #),\n",
    "        tokenizer.encode(\n",
    "            clean_text(\"Annuler l’Euro 2020 ferait perdre très gros à l’UEFA\"),\n",
    "            add_special_tokens=True\n",
    "        )\n",
    "    ], tf.int32\n",
    ")\n",
    "out = model(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pNqWse-4emRO"
   },
   "outputs": [],
   "source": [
    "print(out)\n",
    "print(np.argmax(out[0]))\n",
    "print(list(labels)[np.argmax(out[0])])\n",
    "\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "c, d = file_generator(2)\n",
    "#Confution Matrix and Classification Report\n",
    "Y_pred = model(c)\n",
    "#print(Y_pred)\n",
    "y_pred = [labels[int(np.argmax(y))] for y in Y_pred[0]]\n",
    "d = enc.inverse_transform(d)\n",
    "\n",
    "print(Y_pred)\n",
    "print(y_pred)\n",
    "print(d)\n",
    "\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(d, y_pred))\n",
    "print('Classification Report')\n",
    "target_names = ['Cats', 'Dogs', 'Horse']\n",
    "print(classification_report(d, y_pred, target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Import && train model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
