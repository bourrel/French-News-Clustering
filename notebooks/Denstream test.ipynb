{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import csv\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "import numpy as np\n",
    "from time import time, mktime\n",
    "from datetime import datetime, timedelta\n",
    "from random import randint\n",
    "from uuid import UUID\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import (\n",
    "    TFFlaubertForSequenceClassification,\n",
    "    FlaubertTokenizer,\n",
    "    FlaubertConfig\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../flaubert_token_classification.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = FlaubertTokenizer.from_pretrained(\"jplu/tf-flaubert-base-cased\")\n",
    "model = TFFlaubertForTokenClassification.from_pretrained(\"../models/ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Article():\n",
    "    def __init__(self, raw, token, date, entities):\n",
    "        self.cluster = None\n",
    "        self.raw = raw\n",
    "        self.token = token\n",
    "        self.date = int(date)\n",
    "        self.entities = entities\n",
    "        \n",
    "    def set_cluster(self, cluster):\n",
    "        self.cluster = cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f807ffdcfec49ffa991b96006ef699b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import denstream\n",
    "\n",
    "dataset_ = open(\"../dataset/custom_dataset/since_january.csv\")\n",
    "\n",
    "sentences = []\n",
    "reader = csv.reader(dataset_, delimiter=',', quotechar='\"')\n",
    "\n",
    "next(reader) # Skip header\n",
    "\n",
    "for idx, line in tqdm(enumerate(reader)):\n",
    "    if idx > 1001:\n",
    "        break\n",
    "    if len(line) < 3 or line[1] == \"\":\n",
    "        continue\n",
    "\n",
    "    article = line[0]\n",
    "    tokens = tokenizer.encode(article, max_length=SEQUENCE_LENGTH, pad_to_max_length=SEQUENCE_LENGTH, add_special_tokens=True, return_tensors='tf')\n",
    "    transformer_outputs = model.transformer(tokens)[0][0]\n",
    "    token_classification_outputs = model(tokens)[0]\n",
    "    token_classification_outputs = np.argmax(token_classification_outputs, axis=2)[0]\n",
    "\n",
    "    # Get entities tokens\n",
    "    entities = []\n",
    "    for idx, entity in enumerate(token_classification_outputs):\n",
    "        if entity != 8:\n",
    "            entities.append(tokens[0][idx].numpy())\n",
    "\n",
    "    sentences.append(\n",
    "        Article(\n",
    "            raw=article,\n",
    "            token=transformer_outputs[0],\n",
    "            date=datetime.strptime(line[1], \"%Y-%m-%d %H:%M:%S\").timestamp(),\n",
    "            entities=entities\n",
    "        )\n",
    "    )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = sentences[0]\n",
    "sentences = sentences[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Denstream algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from denstream import Sample, OutlierDenStream\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = OutlierDenStream(lamb=2, startingBuffer=[document.token.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo.runInitialization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leo/Work/eperf_consulting/bert_clustering/notebooks/denstream.py:144: RuntimeWarning: invalid value encountered in sqrt\n",
      "  maxRad = np.nanmax(np.sqrt(SSd.astype(float)-LSd.astype(float)))\n"
     ]
    }
   ],
   "source": [
    "for idx, sentence in enumerate(sentences):\n",
    "    algo.runOnNewSample(Sample(sentence.token.numpy(), idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Clusters: 15\n",
      "-----\n",
      "Cluster #0\n",
      "Samples: 3\n",
      "Weight: 5.527486543369371e-76\n",
      "Creation Time: 280\n",
      "LastEdit Time: 291\n",
      "Cluster #1\n",
      "Samples: 19\n",
      "Weight: 4.280599731007093e-50\n",
      "Creation Time: 193\n",
      "LastEdit Time: 334\n",
      "Cluster #2\n",
      "Samples: 2\n",
      "Weight: 1.2622544855513944e-29\n",
      "Creation Time: 361\n",
      "LastEdit Time: 368\n",
      "Cluster #3\n",
      "Samples: 4\n",
      "Weight: 5.364273414875793e-29\n",
      "Creation Time: 351\n",
      "LastEdit Time: 369\n",
      "Cluster #4\n",
      "Samples: 6\n",
      "Weight: 8.411152611240296e-25\n",
      "Creation Time: 363\n",
      "LastEdit Time: 376\n",
      "Cluster #5\n",
      "Samples: 4\n",
      "Weight: 8.503965996670648e-22\n",
      "Creation Time: 372\n",
      "LastEdit Time: 381\n",
      "Cluster #6\n",
      "Samples: 6\n",
      "Weight: 1.3877787807814457e-17\n",
      "Creation Time: 301\n",
      "LastEdit Time: 388\n",
      "Cluster #7\n",
      "Samples: 2\n",
      "Weight: 5.55247037584139e-17\n",
      "Creation Time: 383\n",
      "LastEdit Time: 389\n",
      "Cluster #8\n",
      "Samples: 11\n",
      "Weight: 5.684341886080802e-14\n",
      "Creation Time: 242\n",
      "LastEdit Time: 394\n",
      "Cluster #9\n",
      "Samples: 5\n",
      "Weight: 3.814697265625e-06\n",
      "Creation Time: 296\n",
      "LastEdit Time: 407\n",
      "Cluster #10\n",
      "Samples: 6\n",
      "Weight: 0.00024414062500089186\n",
      "Creation Time: 375\n",
      "LastEdit Time: 410\n",
      "Cluster #11\n",
      "Samples: 12\n",
      "Weight: 0.0009765625000035527\n",
      "Creation Time: 332\n",
      "LastEdit Time: 411\n",
      "Cluster #12\n",
      "Samples: 9\n",
      "Weight: 0.003922482094139923\n",
      "Creation Time: 395\n",
      "LastEdit Time: 412\n",
      "Cluster #13\n",
      "Samples: 22\n",
      "Weight: 0.2500610949982873\n",
      "Creation Time: 320\n",
      "LastEdit Time: 415\n",
      "Cluster #14\n",
      "Samples: 3\n",
      "Weight: 1.015625238418579\n",
      "Creation Time: 405\n",
      "LastEdit Time: 416\n"
     ]
    }
   ],
   "source": [
    "algo.pMicroCluster.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n",
      "0.0005787647715686867\n",
      "53 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for a in algo.oMicroCluster.clusters:\n",
    "    print(len(a.getCenter()))\n",
    "    print(a.getRadius())\n",
    "    print(a.clusterNumber, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
